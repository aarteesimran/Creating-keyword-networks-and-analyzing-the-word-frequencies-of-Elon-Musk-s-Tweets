---
title: "Creating keyword networks and analyzing the word frequencies of Elon Musk’s Tweets"
author: "Aartee Simran Dhomeja"
date: "12/15/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Task 1
```{r}
library(igraph)
#Loading the Libraries
library(dplyr)
library(stringr)
library(tidytext)
library(janeaustenr)
library(ggplot2)
library(tidyr)
library(igraph)
library(ggraph)
library(plotly)
```
Task - 1 deals with the keyword network analysis matrix and operations on the same. We will be using iGraph functions and converting the matrix into a weighted directed network. Next we've computed the node strength and degree and the maximum weights for the same. Lastly a graph has been plotted to depict the association between degrees and average strength.
#part a: Use the solution from homework 1 question 3.1 as the adjacency matrix
```{r}
#part a: Use the solution from homework 1 question 3.1 as the adjacency matrix
# Stack all variables to find unique
Keyword_data<-read.csv("C:/Users/aartee/Downloads/Keyword_data.csv")
wordstack<-stack(Keyword_data)
# Calculate unique keywords
unkeyword<-unique(wordstack$values)
# Create a weighted adjacency matrix
keyanswer<-matrix(0, nrow=length(unkeyword), ncol=length(unkeyword))
colnames(keyanswer)<-unkeyword
rownames(keyanswer)<-unkeyword
Keyword_data$Keyword.2
# Logic to create weighted matrix
for(i in 1:length(Keyword_data$Keyword.2)){
  temp<-unlist(Keyword_data[i,])
  temp<-temp[!is.na(temp)]
  keyword_list<-combn(temp,2)
  for(j in 1:length(keyword_list[1,])){
    rowind<-which(rownames(keyanswer)==(keyword_list[1,j]))
    colind<-which(colnames(keyanswer)==(keyword_list[2,j]))
    keyanswer[rowind,colind]<-keyanswer[rowind,colind]+1
    keyanswer[colind,rowind]<-keyanswer[colind,rowind]+1
  }
}

max(keyanswer)

#Dropping NA row and column
keyanswer=keyanswer[-157,-157]
```

#Read the adjacency matrix and convert it into a weighted network
```{r}
############task 1 part 2#######################
ans_mat<- graph_from_adjacency_matrix(keyanswer,mode = "undirected",weighted = TRUE)
ans_mat<- graph_from_adjacency_matrix(keyanswer,mode = "directed",weighted = TRUE)
V(ans_mat)$type.label
V(ans_mat)$audience.size
graph_weight <- E(ans_mat)$weight
graph_weight
graph_keyword<-attr(E(ans_mat),"vnames")
weight_key<-data.frame(graph_weight,graph_keyword)
weight_key<-weight_key %>% arrange(desc(graph_weight))
#new_weight<-weight_key %>% top_n(graph_weight,10)
top_weight<-head(weight_key,10)
top_weight
```

#Compute node degree and strength
```{r}
############Degree and Strength######################
degree_graph <- degree(ans_mat, mode="all")
strength_graph<- strength(ans_mat,mode = "all")
typeof(degree_graph)
  #unnaming degree and strength
library(base)

strength<-unname(strength_graph, force = FALSE)
keyword<-names(strength_graph)
db=data.frame(keyword,strength)

degree_1<-unname(degree_graph,force = FALSE)

keyword<-names(degree_graph)
db2=data.frame(keyword,degree_1)
db2
#ans<-data.frame(answer_mat)
#new_graph<-data.frame( )
```

#Show the top 10 nodes by degree and top 10 nodes by strength
```{r}
new_graph<- merge(x=db,y=db2,by="keyword")
#top10_strength<-sort(desc(new_graph$strength))
top10_strength<-new_graph%>%arrange(desc(strength))
top10s<-head(top10_strength,10)
top10s
#head(top10_strength,10)
top10_degree<-new_graph%>% arrange(desc(degree_1))
top10d<-head(top10_degree,10)
top10d
```

#Show the top 10 node pairs by weight
```{r}
weight<-sort(desc(graph_weight))
head(weight,10)
graph_keyword<-attr(E(ans_mat),"vnames")
weight_key<-data.frame(graph_weight,graph_keyword)
weight_key<-weight_key %>% arrange(desc(graph_weight))
#new_weight<-weight_key %>% top_n(graph_weight,10)
top_weight<-head(weight_key,10)
top_weight


cnt<-unique(new_graph$degree)
#avgdegreestregth<-data.frame(cnt)
#avgdegreestregth<-sort(desc(avgdegreestregth$))
#colnames(avgdegreestregth)
```

#Plot average strength on y-axis and degree on x-axis
```{r}
unique_degree=unique(new_graph$degree_1)
#print(b)
degree_strength1<-new_graph %>% group_by(degree_1) %>% summarise(sum(strength))
degree_count<-new_graph %>% group_by(degree_1) %>% count(degree_1)

colnames(degree_strength1)=c("degree", "sumofstrength")
colnames(degree_count)=c("degree","countofstrength")
final_df = merge(x=degree_strength1,y=degree_count,by="degree") %>% mutate(avg_strength=sumofstrength/countofstrength)
final_df
plot_avg<-plot_ly(final_df,x=~degree,y=~avg_strength,marker=list(size=5))
plot_avg
#plot(df3$degree,df3$avg_strength)
 
```
Conclusion: The above pieces of work showcase the keyword network analysis, conversion of matrix to network. Through this project we've learnt calculating node weights, strength, and degree, use of igraph to calculate various values. The end result shows the value of degree and its associated average strength. The frequency of occurrence is higher is lower and the average strength is higher where lower degrees occur more frequently and have lower average strength.

```{r}
final_df$avg_str<-final_df$sumofstrength/final_df$degree
#new_graph$avg_str<- new_graph$strength/new_graph$degree
plot_avg<-plot_ly(final_df,x=~degree,y=~avg_str,marker=list(size=5))
plot_avg<-plot_avg %>% layout(title="Plot for Average Strength")
plot_avg
```


#Task 2
In the second task of this project we analyze Elon Musk's tweets from 2010-2021. The analyses is a combination of removal of stop words, analyzing the frequently used words, showing the top 10 words used each year. Next we've used Zipf's Law for log plot and word frequencies and ranks for each year. In the end we have plotted bigram network graph each year.
```{r}
#Loading the Data set
#Data set
Tweet <- read.csv("C:/Users/aartee/Downloads/archive/2021.csv")
```

```{r}
#Sub-setting and making Final Data set

#Sub-setting
df_tweet <- Tweet[c(5,8)]

#Separating 
separate_df <- separate(df_tweet, col = date, into = c("Year", "Month", "Date"), sep = "-") 
separate_df <- separate_df[c(1,4)]

Final_Tweet_df <- separate_df 
```

```{r}

#Filtering the Year
Final_Tweet_df_2017 <- Final_Tweet_df %>% filter(Year == "2017")

#Making each tweet as each line count
Final_Tweet_df_2017_Tibbled <- tibble(line = 1:1161, text = Final_Tweet_df_2017$tweet)

# Words as tokens
Final_Tweet_df_2017_token <- Final_Tweet_df_2017_Tibbled %>%
  unnest_tokens(word, text)

#Add some custom stop words based on problem context
word_n <- c("http", "https", "t.co", "amp", "it's", "â")
lexicon <- c("custom", "custom", "custom", "custom", "custom", "custom")
df <- data.frame(word_n,lexicon)
stop_words <- full_join(df, stop_words)

#Remove stop words from the tweets  
Final_Tweet_df_2017_token <- anti_join(Final_Tweet_df_2017_token, stop_words)

# Word frequency for Repeating Words
Tweet_words_2017 <- Final_Tweet_df_2017_token %>%
  count(word, sort = TRUE)

#Top -10 word frequency for Repeating Words
Tweet_words_2017_top10 <- Final_Tweet_df_2017_token %>%
  count(word, sort = TRUE) %>% head(10)
Tweet_words_2017_top10

# Word frequency for Non - Repeating Words
Total_Tweet_words_2017 <- Final_Tweet_df_2017_token %>% count(word, sort = TRUE)

#Counting the Total Words
Total_Tweet_words_2017$Total_Words <- sum(Total_Tweet_words_2017$n) 

#Top 10 Words with total words
Total_Tweet_words_2017_top_10 <- head(Total_Tweet_words_2017, 10)
Total_Tweet_words_2017_top_10

```


```{r}
#Plot a histogram of the most commonly tweeted words by Musk in 2017

#Plot a histogram of the most commonly tweeted words by Musk in 2017 above 15

plt_2017_a <- Total_Tweet_words_2017 %>%
  filter(n > 15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2017 Tweets above 15 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2017_a

#Plot a histogram of the most commonly tweeted words by Musk in 2017 above 30

plt_2017_b <- Total_Tweet_words_2017 %>%
  filter(n > 30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2017 Tweets above 30 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2017_b


```

```{r}
# Histogram for Total frequency words 

tweets_2017_total <- Total_Tweet_words_2017 %>%
  count(word, sort = TRUE) 
ggplot(Total_Tweet_words_2017, aes(n/Total_Words), fill = word) + 
  geom_histogram(show.legend = FALSE, color = "red", fill = "blue") +
  xlim(NA, 0.009) +
  labs(title = "Total Word Frequencies in 2017")

```

```{r}
#Zipf's law for 2017 twitter data (Total Words)
freq_by_rank_2017_TotaL_words <- Total_Tweet_words_2017 %>%
  mutate(rank = row_number(), `term frequency` = n/Total_Words) %>%
  ungroup()

plt_2_2017_Total_Words <- freq_by_rank_2017_TotaL_words %>%
  ggplot(aes(rank, `term frequency`)) +
  geom_line(size = 1.1, alpha = .8, show.legend = FALSE, 
            color = "blue") +
  scale_x_log10() +
  scale_y_log10()+
  ggtitle("Zipf's Law Graph for Frequency Rank of Year 2017")
plt_2_2017_Total_Words

```

```{r}
#Creating Bigrams

tweets_2017_bigrams <- Final_Tweet_df_2017_Tibbled %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

tweets_2017_bigrams_separated <- tweets_2017_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

tweets_2017_bigrams_filtered <- tweets_2017_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

bigram_counts_2017 <- na.omit(tweets_2017_bigrams_filtered) %>%
  count(word1, word2, sort = TRUE)

bigram_counts_2017 %>%
  head(10)

bigram_2017_graph <- bigram_counts_2017 %>%
  filter(n > 2) %>%
  graph_from_data_frame()

set.seed(2017)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2017_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 2 for year 2017")

```
Conclusion: After undertaking several analysis we found the most frequently occurring words were https, teals, 3 and spaceX etc. The frequency histogram further depicts the frequency of occurrence of words the top 15 and top 30. The total frequency histogram shows count/total words. The bigram plotted for the same takes in account the condition of count n greater than 2

2018

```{r}
#Filtering Each year
Final_Tweet_df_2018 <- Final_Tweet_df %>% filter(Year == "2018")

#Making each tweet as each line count
Final_Tweet_df_2018_Tibbled <- tibble(line = 1:2288, text = Final_Tweet_df_2018$tweet)



# Words as tokens
Final_Tweet_df_2018_token <- Final_Tweet_df_2018_Tibbled %>%
  unnest_tokens(word, text)

#Add some custom stop words based on problem context
word_n <- c("http", "https", "t.co", "amp", "it's", "â", "ðÿ", "itâ", "donâ", "youâ", "ï", "iâ", "weâ")
lexicon <- c("custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom")
df <- data.frame(word_n,lexicon)
stop_words <- full_join(df, stop_words)


#Remove stop words from the tweets  
Final_Tweet_df_2018_token <- anti_join(Final_Tweet_df_2018_token, stop_words)


# Word frequency
Tweet_words_2018 <- Final_Tweet_df_2018_token %>%
  count(word, sort = TRUE)

#Top -10 Words
Tweet_words_2018_top10 <- Final_Tweet_df_2018_token %>%
  count(word, sort = TRUE) %>% head(10)
Tweet_words_2018_top10

# Word frequency for Non - Repeating Words
Total_Tweet_words_2018 <- Final_Tweet_df_2018_token %>% count(word, sort = TRUE)

#Counting the Total Words
Total_Tweet_words_2018$Total_Words <- sum(Total_Tweet_words_2018$n) 

#Top 10 Words with Words
Total_Tweet_words_2018_top_10 <- head(Total_Tweet_words_2018, 10)
Total_Tweet_words_2018_top_10

```

```{r}
#Plot a histogram of the most commonly tweeted words by Musk in 2018

#Plot a histogram of the most commonly tweeted words by Musk in 2018 above 30

plt_2018_a <- Total_Tweet_words_2018 %>%
  filter(n > 30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2018 Tweets above 30 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2018_a

# Plot a histogram of the most commonly tweeted words by Musk in 2018 above 50

plt_2018_b <- Total_Tweet_words_2018 %>%
  filter(n > 50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2018 Tweets above 50 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2018_b

```

```{r}
# Histogram for Total frequency words 

tweets_2018_total <- Total_Tweet_words_2018 %>%
  count(word, sort = TRUE) 
#Tweet_words_2017$total <- nrow(Tweet_words_2017)
ggplot(Total_Tweet_words_2018, aes(n/Total_Words), fill = word) + 
  geom_histogram(show.legend = FALSE, color = "red", fill = "blue") +
  xlim(NA, 0.009) +
  labs(title = "Total Word Frequencies in 2018")

```

```{r}
#Zipf's law for 2018 twitter data (Total Words)
freq_by_rank_2018_TotaL_words <- Total_Tweet_words_2018 %>%
  mutate(rank = row_number(), `term frequency` = n/Total_Words) %>%
  ungroup()

plt_2_2018_Total_Words <- freq_by_rank_2018_TotaL_words %>%
  ggplot(aes(rank, `term frequency`)) +
  geom_line(size = 1.1, alpha = .8, show.legend = FALSE, 
            color = "blue") +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Zipf's Law Graph for Frequency Rank of Year 2018")
plt_2_2018_Total_Words

```

```{r}
#Creating Bigrams

tweets_2018_bigrams <- Final_Tweet_df_2018_Tibbled %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

tweets_2018_bigrams_separated <- tweets_2018_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

tweets_2018_bigrams_filtered <- tweets_2018_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

bigram_counts_2018 <- na.omit(tweets_2018_bigrams_filtered) %>%
  count(word1, word2, sort = TRUE)

bigram_counts_2018 %>%
  head(10)

#Using Bigrams Graph for word count above 5

bigram_2018_graph <- bigram_counts_2018 %>%
  filter(n > 5) %>%
  graph_from_data_frame()

set.seed(2018)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2018_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 5 for year 2018")

#Using Bigrams Graph for word count above 7
bigram_2018_graph <- bigram_counts_2018 %>%
  filter(n > 7) %>%
  graph_from_data_frame()

set.seed(2018)

b <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2018_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 7 for year 2018")

```
Conclusion: After undertaking several analysis we found the most frequently occurring words were https, tesla, 3 and model etc. The frequency histogram further depicts the frequency of occurrence of words greater than 15 and 30. The total frequency histogram shows count/total words. The bigram plotted for the same takes in account the condition of count n greater than 5 and 7.

2019
```{r}
#Filtering the Year
Final_Tweet_df_2019 <- Final_Tweet_df %>% filter(Year == "2019")

#Making each tweet as each line
Final_Tweet_df_2019_Tibbled <- tibble(line = 1:2932, text = Final_Tweet_df_2019$tweet)

# Words as tokens
Final_Tweet_df_2019_token <- Final_Tweet_df_2019_Tibbled %>%
  unnest_tokens(word, text)

#Add some custom stop words based on problem context
word_n <- c("http", "https", "t.co", "amp", "it's", "â", "ðÿ", "itâ", "donâ", "youâ", "ï", "iâ", " ", "  ", "ð")
lexicon <- c("custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom")
df <- data.frame(word_n,lexicon)
stop_words <- full_join(df, stop_words)

#Remove stop words from the tweets  
Final_Tweet_df_2019_token <- anti_join(Final_Tweet_df_2019_token, stop_words)


# Word frequency
Tweet_words_2019 <- Final_Tweet_df_2019_token %>%
  count(word, sort = TRUE)

#Top -10
Tweet_words_2019_top10 <- Final_Tweet_df_2019_token %>%
  count(word, sort = TRUE) %>% head(10)
Tweet_words_2019_top10

# Word frequency for Non - Repeating Words
Total_Tweet_words_2019 <- Final_Tweet_df_2019_token %>% count(word, sort = TRUE)

#Counting the Total words
Total_Tweet_words_2019$Total_Words <- sum(Total_Tweet_words_2019$n) 

#Top -10 

Total_Tweet_words_2019_top_10 <- head(Total_Tweet_words_2019, 10)
Total_Tweet_words_2019_top_10

```

```{r}
#Plot a histogram of the most commonly tweeted words by Musk in 2019

plt_2019 <- Tweet_words_2019 %>%
  filter(n > 75) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2019 Tweets above 75 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2019
```


```{r}
# Histogram for Total frequency words 

#Plot a histogram of the most commonly tweeted words by Musk in 2019 above 40 count 

plt_2019_a <- Tweet_words_2019 %>%
  filter(n > 40) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2019 Tweets above 40 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2019_a

#Plot a histogram of the most commonly tweeted words by Musk in 2019 above 75 count 

plt_2019_b <- Tweet_words_2019 %>%
  filter(n > 75) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2019 Tweets above 75 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2019_b
```

```{r}
#Zipf's law for 2018 twitter data (Total Words)
freq_by_rank_2019_TotaL_words <- Total_Tweet_words_2019 %>%
  mutate(rank = row_number(), `term frequency` = n/Total_Words) %>%
  ungroup()

plt_2_2019_Total_Words <- freq_by_rank_2019_TotaL_words %>%
  ggplot(aes(rank, `term frequency`)) +
  geom_line(size = 1.1, alpha = .8, show.legend = FALSE, 
            color = "blue") +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Zipf's Law Graph for Frequency Rank of Year 2019")
plt_2_2019_Total_Words
```


```{r}
#Creating Bigrams

tweets_2019_bigrams <- Final_Tweet_df_2019_Tibbled %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

tweets_2019_bigrams_separated <- tweets_2019_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

tweets_2019_bigrams_filtered <- tweets_2019_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

bigram_counts_2019 <- na.omit(tweets_2019_bigrams_filtered) %>%
  count(word1, word2, sort = TRUE)

bigram_counts_2019 %>%
  head(10)

#Bigram graph dor word count above 7

bigram_2019_graph <- bigram_counts_2019 %>%
  filter(n > 7) %>%
  graph_from_data_frame()

set.seed(2019)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2019_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 7 for year 2019")


#Bigram graph dor word count above 10

bigram_2019_graph <- bigram_counts_2019 %>%
  filter(n > 10) %>%
  graph_from_data_frame()

set.seed(2019)

b <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2019_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 10 for year 2019")


```
Conclusion: After undertaking several analysis we found the most frequently occurring words were https, amp, tesla and spaceX etc. The frequency histogram further depicts the frequency of occurrence of words greater than 75 and 40. The total frequency histogram shows count/total words. The bigram plotted for the same takes in account the condition of count n greater than 7 and 10.

2020
```{r}

#Filteting the Year
Final_Tweet_df_2020 <- Final_Tweet_df %>% filter(Year == "2020")

#Making each tweet as each line count
Final_Tweet_df_2020_Tibbled <- tibble(line = 1:3367, text = Final_Tweet_df_2020$tweet)

# Words as tokens
Final_Tweet_df_2020_token <- Final_Tweet_df_2020_Tibbled %>%
  unnest_tokens(word, text)

#Add some custom stop words based on problem context
word_n <- c("http", "https", "t.co", "amp", "it's", "â", "ðÿ", "itâ", "donâ", "youâ", "ï", "iâ")
lexicon <- c("custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom")
df <- data.frame(word_n,lexicon)
stop_words <- full_join(df, stop_words)


#Remove stop words from the tweets  
Final_Tweet_df_2020_token <- anti_join(Final_Tweet_df_2020_token, stop_words)

# Word frequency
Tweet_words_2020 <- Final_Tweet_df_2020_token %>%
  count(word, sort = TRUE)

#Top -10
Tweet_words_2020_top10 <- Final_Tweet_df_2020_token %>%
  count(word, sort = TRUE) %>% head(10)
Tweet_words_2020_top10

# Word frequency for Non - Repeating Words
Total_Tweet_words_2020 <- Final_Tweet_df_2020_token %>% count(word, sort = TRUE)

#Counting the Total Words
Total_Tweet_words_2020$Total_Words <- sum(Total_Tweet_words_2020$n) 

#Top 10 Total Words with Total
Total_Tweet_words_2020_top_10 <- head(Total_Tweet_words_2020,10)
Total_Tweet_words_2020_top_10
```


```{r}
#Plot a histogram of the most commonly tweeted words by Musk in 2020


#Plot a histogram of the most commonly tweeted words by Musk in 2020 above 50 count

plt_2020_a <- Tweet_words_2020 %>%
  filter(n > 50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2020 Tweets above 70 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2020_a

#Plot a histogram of the most commonly tweeted words by Musk in 2020 above 100 count

plt_2020_b <- Tweet_words_2020 %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2020 Tweets above 100 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2020_b
```


```{r}
# Histogram for Total frequency words 

tweets_2020_total <- Total_Tweet_words_2020 %>%
  count(word, sort = TRUE) 
ggplot(Total_Tweet_words_2020, aes(n/Total_Words), fill = word) + 
  geom_histogram(show.legend = FALSE, color = "red", fill = "blue") +
  xlim(NA, 0.009) +
  labs(title = "Total Word Frequencies in 2020")
```


```{r}
#Zipf's law for 2020 twitter data (Total Words)
freq_by_rank_2020_TotaL_words <- Total_Tweet_words_2020 %>%
  mutate(rank = row_number(), `term frequency` = n/Total_Words) %>%
  ungroup()

plt_2_2020_Total_Words <- freq_by_rank_2020_TotaL_words %>%
  ggplot(aes(rank, `term frequency`)) +
  geom_line(size = 1.1, alpha = .8, show.legend = FALSE, 
            color = "blue") +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Zipf's Law Graph for Frequency Rank of Year 2020")
plt_2_2020_Total_Words
```


```{r}
#Creating Bigrams

tweets_2020_bigrams <- Final_Tweet_df_2020_Tibbled %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

tweets_2020_bigrams_separated <- tweets_2020_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

tweets_2020_bigrams_filtered <- tweets_2020_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

bigram_counts_2020 <- na.omit(tweets_2020_bigrams_filtered) %>%
  count(word1, word2, sort = TRUE)

bigram_counts_2020 %>%
  head(10)

#Bigram graph for words above 7 count
bigram_2020_graph <- bigram_counts_2020 %>%
  filter(n > 7) %>%
  graph_from_data_frame()

set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2020_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 7 for year 2020")


#Bigram graph for words above 10 count
bigram_2020_graph <- bigram_counts_2020 %>%
  filter(n > 10) %>%
  graph_from_data_frame()

set.seed(2020)

b <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2020_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 10 for year 2020")


```
Conclusion: After undertaking several analysis we found the most frequently occurring words were https, amp, tesla and spacex etc. The frequency histogram further depicts the frequency of occurrence of words greater than 70 and 100. The total frequency histogram shows count/total words. The bigram plotted for the same takes in account the condition of count n greater than 7 and 10.

2021
```{r}
#Filtering the Year
Final_Tweet_df_2021 <- Final_Tweet_df %>% filter(Year == "2021")

#Making each tweet as each line
Final_Tweet_df_2021_Tibbled <- tibble(line = 1:585, text = Final_Tweet_df_2021$tweet)

# Words as tokens
Final_Tweet_df_2021_token <- Final_Tweet_df_2021_Tibbled %>%
  unnest_tokens(word, text)

#Add some custom stop words based on problem context
word_n <- c("http", "https", "t.co", "amp", "it's", "â", "ðÿ", "itâ", "donâ", "youâ", "ï", "iâ")
lexicon <- c("custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom", "custom")
df <- data.frame(word_n,lexicon)
stop_words <- full_join(df, stop_words)


#Remove stop words from the tweets  
Final_Tweet_df_2021_token <- anti_join(Final_Tweet_df_2021_token, stop_words)

# Word frequency
Tweet_words_2021 <- Final_Tweet_df_2021_token %>%
  count(word, sort = TRUE)

#Top -10 with Word Count
Tweet_words_2021_top10 <- Final_Tweet_df_2021_token %>%
  count(word, sort = TRUE) %>% head(10)
Tweet_words_2021_top10

# Word frequency for Non - Repeating Words
Total_Tweet_words_2021 <- Final_Tweet_df_2021_token %>% count(word, sort = TRUE)

#Counting the Total Words
Total_Tweet_words_2021$Total_Words <- sum(Total_Tweet_words_2021$n) 

#Top-10 with Total Words
Total_Tweet_words_2021_top_10 <-head(Total_Tweet_words_2021,10)
Total_Tweet_words_2021_top_10
```


```{r}
#Plot a histogram of the most commonly tweeted words by Musk in 2021

#Plot a histogram of the most commonly tweeted words by Musk in 2021 above 9 counts

plt_2021_a <- Total_Tweet_words_2021 %>%
  filter(n > 9) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2021 Tweets above 9 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2021_a

#Plot a histogram of the most commonly tweeted words by Musk in 2021 above 15 counts

plt_2021_b <- Total_Tweet_words_2021 %>%
  filter(n > 15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(title = "Musk 2021 Tweets above 15 counts", y = "Word", x = "Frequency") +
  theme(axis.text.y = element_text(face = "bold" , color ="blue", size = 10), 
        axis.text.x = element_text(face="bold",color = "red", size = 10)) +
  geom_bar(stat="identity", color = "red", fill = "blue")

plt_2021_b
```


```{r}
# Histogram for Total frequency words 

tweets_2021_total <- Total_Tweet_words_2021 %>%
  count(word, sort = TRUE) 
ggplot(Total_Tweet_words_2021, aes(n/Total_Words), fill = word) + 
  geom_histogram(show.legend = FALSE, color = "red", fill = "blue") +
  xlim(NA, 0.009) +
  labs(title = "Total Word Frequencies in 2021")
```


```{r}
#Zipf's law for 2021 twitter data (Total Words)
freq_by_rank_2021_TotaL_words <- Total_Tweet_words_2021 %>%
  mutate(rank = row_number(), `term frequency` = n/Total_Words) %>%
  ungroup()

plt_2_2020_Total_Words <- freq_by_rank_2021_TotaL_words %>%
  ggplot(aes(rank, `term frequency`)) +
  geom_line(size = 1.1, alpha = .8, show.legend = FALSE, 
            color = "blue") +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Zipf's Law Graph for Frequency Rank of Year 2021")
plt_2_2020_Total_Words
```


```{r}
#Creating Bigrams

tweets_2021_bigrams <- Final_Tweet_df_2021_Tibbled %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

tweets_2021_bigrams_separated <- tweets_2021_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

tweets_2021_bigrams_filtered <- tweets_2021_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

bigram_counts_2021 <- na.omit(tweets_2021_bigrams_filtered) %>%
  count(word1, word2, sort = TRUE)

bigram_counts_2021 %>%
  head(10)

#Bigram graph for word counts above 2 
bigram_2021_graph <- bigram_counts_2021 %>%
  filter(n > 2) %>%
  graph_from_data_frame()

set.seed(2021)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2021_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 2for year 2021")

#Bigram graph for word counts above 3
bigram_2021_graph <- bigram_counts_2021 %>%
  filter(n > 3) %>%
  graph_from_data_frame()

set.seed(2021)

b <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_2021_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,
                 end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Bigrams Words Count Above 3 for year 2021")

```
Conclusion: After undertaking several analysis we found the most frequently occurring words were https, tesla, 3 and model etc. The frequency histogram further depicts the frequency of occurrence of words greater than 9 and 15. The total frequency histogram shows count/total words. The bigram plotted for the same takes in account the condition of count n greater than 2 and 3.

